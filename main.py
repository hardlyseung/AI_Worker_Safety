import cv2
import mediapipe as mp  # # PoseLandmark ì´ë¦„ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í•„ìš”
import time
import os
import sys # ì‹œìŠ¤í…œ ì •ë³´ (íŒŒì´ì¬ ë²„ì „ ë“±) í™•ì¸ìš©
import tqdm
import numpy as np      # NumPy: ë‹¤ì°¨ì› ë°°ì—´, í–‰ë ¬ ì—°ì‚°
import pandas as pd     # Pandas: ë°ì´í„° ì¡°ì‘, ë¶„ì„
import glob             # Glob: íŒŒì¼ ê²½ë¡œ íŒ¨í„´ ë§¤ì¹­
import torch            # PyTorch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import torch.nn as nn   # PyTorch: ì‹ ê²½ë§ ëª¨ë“ˆ
import torch.optim as optim # PyTorch: ìµœì í™” ì•Œê³ ë¦¬ì¦˜
import tensorflow as tf, numpy as np, datetime, os
import matplotlib.pyplot as plt # ì„ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ì‹œê°í™”
from collections import deque
import subprocess
import RPi.GPIO as GPIO
import time
import platform

from model_definition import LSTMAutoencoder  # ì˜¤í† ì¸ì½”ë” ì •ì˜ import


#ì˜ìƒ ê·œê²©
WIDTH, HEIGHT = 640, 480

#------------------ YOLOv5 ëª¨ë¸ ë¡œë“œ  ------------------
yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) # v5sëª¨ë¸ ì”€
yolo_model.classes = [0]  # ì‚¬ëŒë§Œ ê°ì§€
yolo_model.conf = 0.5

#model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)
#model.conf = 0.5
#model.classes = [0]  # ì‚¬ëŒë§Œ ê°ì§€ 

#------------------ LED ë° ë¶€ì € í•¨ìˆ˜ ------------------
BUZZER_PIN = 17  # ë¶€ì € ì—°ê²° í•€
#LED_PIN = 27 # LED ì—°ê²° í•€
GPIO.setmode(GPIO.BCM)
GPIO.setup(BUZZER_PIN, GPIO.OUT)
#GPIO.setup(LED_PIN, GPIO.OUT)

def alert_buzzer_and_led():
    GPIO.output(BUZZER_PIN, GPIO.HIGH)
#    GPIO.output(LED_PIN, GPIO.HIGH)
    time.sleep(0.3)
    GPIO.output(BUZZER_PIN, GPIO.LOW)
#    GPIO.output(LED_PIN, GPIO.LOW)

def extract_20joints_from_landmarks(landmarks, joint_map):
    joint_data = []
    for joint_name, info in joint_map.items():
        if isinstance(info, tuple) and info[0] == "avg":
            indices = info[1]
            if all(i < len(landmarks) for i in indices):
                joint = np.mean([landmarks[i] for i in indices], axis=0)
            else:
                joint = np.array([-1.0, -1.0, -1.0])
        else:
            if info < len(landmarks):
                joint = landmarks[info]
            else:
                joint = np.array([-1.0, -1.0, -1.0])
        joint_data.extend(joint.tolist())
    return joint_data

#------------------ MediaPipe ì´ˆê¸°í™” ------------------
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils
max_frames = 30     # 30í”„ë ˆì„ (25~30ìœ¼ë¡œ ìœ ë™ì  ì¡°ì • ê°€ëŠ¥)
pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, min_detection_confidence=0.5)

# ê´€ì ˆ ì¸ë±ìŠ¤ê°€ ë¶€ì¡±í•  ì‹œ -1.0ìœ¼ë¡œ ì²˜ë¦¬
def extract_20joints_from_landmarks(landmarks, joint_map):
    joint_data = []
    for joint_name, info in joint_map.items():
        if isinstance(info, tuple) and info[0] == "avg":
            indices = info[1]
            if all(i < len(landmarks) for i in indices):
                joint = np.mean([landmarks[i] for i in indices], axis=0)
            else:
                joint = np.array([-1.0, -1.0, -1.0])
        else:
            if info < len(landmarks):
                joint = landmarks[info]
            else:
                joint = np.array([-1.0, -1.0, -1.0])
        joint_data.extend(joint.tolist())
    return joint_data

#------------------ ê´€ì ˆ ë§¤í•‘ (20ê°œ) ------------------
mediapipe_index = {
    "Head": 0,
    "Shouldercentre": ("avg", [11, 14]),
    "Spine": ("avg", [27, 28]),
    "Leftshoulder": 14,
    "Leftelbow": 15,
    "Lefthand": 16,
    "Rightshoulder": 11,
    "Rightelbow": 12,
    "Righthand": 13,
    "Lefthip": 27,
    "Leftknee": 29,
    "Leftfoot": 31,
    "Righthip": 28,
    "Rightknee": 30,
    "Rightfoot": 32,
    "Hipcentre": ("avg", [27, 28]),
    "Leftwrist": 16,
    "Leftankle": 31,
    "Rightwrist": 13,
    "Rightankle": 32
}

#------------------ í”„ë ˆì„ ì¶”ì¶œ ------------------
#cap = cv2.VideoCapture(0)# ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ì…ë ¥
#fps = cap.get(cv2.CAP_PROP_FPS)
#interval = int(round(fps / 30))  # 30FPS ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ ê°„ê²© ê³„ì‚°

#frames = []
#frame_ids = []
#count = 0
#while cap.isOpened():
#    ret, frame = cap.read()
#    if not ret:
#        break
#    if count % interval == 0:
#        resized = cv2.resize(frame, (640, 480))
#        frames.append(resized)
#        frame_ids.append(count)
#    count += 1
#cap.release()

# Raspberry Piìš© GStreamer pipeline (PiCam V3 + libcamera)
def gstreamer_pipeline(
    sensor_id=0,
    capture_width=640,
    capture_height=480,
    display_width=640,
    display_height=480,
    framerate=30,
    flip_method=0
):
    return (
        f"libcamerasrc sensor-id={sensor_id} ! "
        f"video/x-raw,width={capture_width},height={capture_height},framerate={framerate}/1 ! "
        f"videoconvert ! videoscale ! "
        f"video/x-raw,width={display_width},height={display_height} ! "
        f"appsink"
    )

# GStreamerë¥¼ í†µí•´ ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„±
cap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)

if not cap.isOpened():
    print("Can't Open Camera")
    exit()

print("Press 'q' to end this program")

frame_count = 0
start_time = time.time()

try:
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Can't read frame")
            break

        # ì‹¤ì‹œê°„ ë””ìŠ¤í”Œë ˆì´ (GUI í™˜ê²½ë§Œ ê°€ëŠ¥)
        #cv2.imshow("ğŸ“· Live View (Raspberry Pi)", frame)

        # 'q' í‚¤ ëˆ„ë¥´ë©´ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        frame_count += 1

except KeyboardInterrupt:
    print("\nUser stop")

finally:
    cap.release()
    cv2.destroyAllWindows()
    elapsed = time.time() - start_time
    print(f"Total {frame_count}frame, average FPS: {frame_count / elapsed:.2f}")

#------------------ LSTM ì˜¤í† ì¸ì½”ë” ------------------
class LSTMAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=2, dropout=0.3, batch_first=True)
        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers=2, dropout=0.3, batch_first=True)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.output_layer = nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        _, (h_n, c_n) = self.encoder(x)
        decoder_input = torch.zeros(batch_size, seq_len, h_n.shape[-1]).to(x.device)
        dec_out, _ = self.decoder(decoder_input, (h_n, c_n))
        dec_out = self.layer_norm(dec_out)  # ì•ˆì •í™”
        out = self.output_layer(dec_out)

        return out

#------------------ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ------------------ 
input_dim = 60  # 20 joints * 3
hidden_dim = 64
seq_len = 30

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMAutoencoder(input_dim, hidden_dim).to(device)
model.load_state_dict(torch.load("/home/py/lstm_autoencoder.pth", map_location=device)) #íŒŒì¼ê²½ë¡œ
model.eval()

#------------------ ìœ„í—˜ ë™ì‘ ê°ì§€ ------------------ 
N_JOINTS = 20  # ê´€ì ˆ ê°œìˆ˜
SEQ_LEN = 30   # 30í”„ë ˆì„
STATE_NORMAL = 0       # ì„œìˆìŒ
STATE_LYING = 1        # ëˆ„ì›ŒìˆëŠ” ìƒíƒœ
STATE_POSSIBLE_FALL = 2  # ë‚™ìƒ ì˜ì‹¬ (ëˆ„ì›Œì„œ ì •ì§€)

def estimate_posture_from_bbox(bbox, standing_thresh=1.2, lying_thresh=0.8):
    x1, y1, x2, y2 = bbox
    width = abs(x2 - x1)
    height = abs(y2 - y1)

    # ì˜ˆì™¸ ì²˜ë¦¬: bboxê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì„ ê²½ìš°
    if width <= 0 or height <= 0:
        return "invalid"

    aspect_ratio = height / width  # ë¹„ìœ¨ ê³„ì‚°

    if aspect_ratio > standing_thresh:
        return "standing"
    elif aspect_ratio < lying_thresh:
        return "lying"
    else:
        return "ambiguous"

def fall_detection_fsm(posture, motion_score, state, counter,
                       motion_thresh=0.01, lying_duration_thresh_sec=4, fps=30):
    
    lying_duration_thresh_frames = lying_duration_thresh_sec * fps
    is_fall = False

    if posture == "lying":
        if motion_score < motion_thresh:
            counter += 1
            if counter >= lying_duration_thresh_frames:
                state = STATE_POSSIBLE_FALL
                is_fall = True
        else:
            counter = 0  # ì›€ì§ì„ì´ ìˆìœ¼ë©´ ì¹´ìš´í„° ì´ˆê¸°í™”
    else:
        state = STATE_NORMAL
        counter = 0

    return state, counter, is_fall

def draw_bbox_and_joints(image, bbox, joints):
    x1, y1, x2, y2 = bbox
    # bbox ê·¸ë¦¬ê¸°
    if x1 >= 0 and y1 >= 0:
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # ê´€ì ˆ ê·¸ë¦¬ê¸°
    if joints is not None and (joints[:, 0] != -1).all():
        h, w = image.shape[:2]
        for x, y, v in joints:
            cx, cy = int(x * w), int(y * h)
            if v > 0.5:  # visibility ì¡°ê±´
                cv2.circle(image, (cx, cy), 3, (0, 0, 255), -1)

    return image

def extract_pose_sequence_npy_from_frames(frames, seq_name, output_dir, save_visual=True):
    os.makedirs(output_dir, exist_ok=True)
    vis_dir = os.path.join(output_dir, "visual")

    person_state = {}
    person_prev_joints = {}
    person_joint_buffer = {}
    people_data = []

    for frame_idx, image in enumerate(frames):
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = yolo_model(image_rgb)
        boxes = results.xyxy[0].cpu().numpy()

        for pid, det in enumerate(boxes):
            if int(det[5]) != 0:
                continue

            x1, y1, x2, y2, _, cls = map(int, det[:6])
            bbox = [x1, y1, x2, y2]
            crop = image_rgb[y1:y2, x1:x2]
            res = pose.process(crop)

            if res.pose_landmarks:
                landmarks = np.array([[lm.x, lm.y, lm.visibility] for lm in res.pose_landmarks.landmark])
                joints = extract_20joints_from_landmarks(landmarks, mediapipe_index)
            else:
                joints = [-1.0] * (N_JOINTS * 3)

            joints_np = np.array(joints).reshape(N_JOINTS, 3)

            if pid not in person_state:
                person_state[pid] = {'state': STATE_NORMAL, 'counter': 0, 'prev_bbox': bbox}
                person_prev_joints[pid] = None
                person_joint_buffer[pid] = []

            # ëª¨ì…˜ìŠ¤ì½”ì–´ ê³„ì‚° (ê´€ì ˆ ê¸°ë°˜)
            if person_prev_joints[pid] is not None and (joints_np[:, 0] != -1.0).all():
                diff = np.linalg.norm(joints_np[:, :2] - person_prev_joints[pid][:, :2], axis=1)
                score = float(np.mean(diff))
            else:
                score = 0.0
            person_prev_joints[pid] = joints_np

            # FSM ìƒíƒœ ì¶”ì •
            posture = estimate_posture_from_bbox(bbox)
            state, counter, is_fall = fall_detection_fsm(posture, score, person_state[pid]['state'], person_state[pid]['counter'])
            person_state[pid]['state'] = state
            person_state[pid]['counter'] = counter
            person_state[pid]['prev_bbox'] = bbox

            if is_fall:
                print(f"likely fall: Frame {frame_idx}, ID={pid}, posture={posture}, motion_score={score:.4f}")

            # ì˜¤í† ì¸ì½”ë” (ë™ì  ìƒíƒœì—ì„œë§Œ)
            static_flag = 0 if score < 0.02 else 1
            if static_flag == 1 and (joints_np[:, 0] != -1.0).all():
                flat = joints_np.flatten()
                person_joint_buffer[pid].append(flat)
                if len(person_joint_buffer[pid]) > SEQ_LEN:
                    person_joint_buffer[pid].pop(0)
                if len(person_joint_buffer[pid]) == SEQ_LEN:
                    input_seq = torch.tensor(person_joint_buffer[pid], dtype=torch.float32).unsqueeze(0).to(device)
                    with torch.no_grad():
                        recon = model(input_seq)
                        recon_error = torch.mean((input_seq - recon) ** 2).item()
                    print(f"Frame {frame_idx}: ID={pid} dynamic state â†’ restoration error MSE = {recon_error:.5f}")
                    if recon_error > 0.01:
                        print(f"Sense abnormal behavior (autoencoder): ID={pid}, MSE = {recon_error:.5f}")
import cv2
import mediapipe as mp  # # PoseLandmark ì´ë¦„ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í•„ìš”
import time
import os
import sys # ì‹œìŠ¤í…œ ì •ë³´ (íŒŒì´ì¬ ë²„ì „ ë“±) í™•ì¸ìš©
import tqdm
import numpy as np      # NumPy: ë‹¤ì°¨ì› ë°°ì—´, í–‰ë ¬ ì—°ì‚°
import pandas as pd     # Pandas: ë°ì´í„° ì¡°ì‘, ë¶„ì„
import glob             # Glob: íŒŒì¼ ê²½ë¡œ íŒ¨í„´ ë§¤ì¹­
import torch            # PyTorch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import torch.nn as nn   # PyTorch: ì‹ ê²½ë§ ëª¨ë“ˆ
import torch.optim as optim # PyTorch: ìµœì í™” ì•Œê³ ë¦¬ì¦˜
import tensorflow as tf, numpy as np, datetime, os
import matplotlib.pyplot as plt # ì„ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ì‹œê°í™”
from collections import deque
import subprocess
import RPi.GPIO as GPIO
import datetime

# í”„ë ˆì„ ì´ˆê¸°í™” ë”•ì…”ë„ˆë¦¬
person_state = {}
person_prev_joints = {}
person_joint_buffer = {}

#-------------------log_print------------------------
last_log_time = time.time()

def log_status_every_3s(frame_idx, detected, person_id=None):
    global last_log_time
    now = time.time()
    if now - last_log_time >= 3:
        timestamp = datetime.datetime.now().strftime('%H:%M:%S')
        if detected:
            print(f"[{timestamp}] Frame {frame_idx}: Person detected  | ID: {person_id}")
        else:
            print(f"[{timestamp}] Frame {frame_idx}: No person detected")
        last_log_time = now


WIDTH, HEIGHT = 640, 480

#------------------ YOLOv5 ëª¨ë¸ ë¡œë“œ  ------------------
yolo_model = torch.hub.load('/home/py/yolov5', 'custom', path= 'yolov5s.pt', source='local') # v5sëª¨ë¸ ì”€
yolo_model.classes = [0]  # ì‚¬ëŒë§Œ ê°ì§€
yolo_model.conf = 0.5

#model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)
#model.conf = 0.5
#model.classes = [0]  # ì‚¬ëŒë§Œ ê°ì§€ 

#------------------ LED ë° ë¶€ì € í•¨ìˆ˜ ------------------
BUZZER_PIN = 17  # ë¶€ì € ì—°ê²° í•€
LED_PIN = 27 # LED ì—°ê²° í•€
GPIO.setmode(GPIO.BCM)
GPIO.setwarnings(False)
GPIO.setup(BUZZER_PIN, GPIO.OUT)
GPIO.setup(LED_PIN, GPIO.OUT)

def alert_buzzer_and_led():
    GPIO.output(BUZZER_PIN, GPIO.HIGH)
    GPIO.output(LED_PIN, GPIO.HIGH)
    time.sleep(0.3)
    GPIO.output(BUZZER_PIN, GPIO.LOW)
    GPIO.output(LED_PIN, GPIO.LOW)


#------------------ MediaPipe ì´ˆê¸°í™” ------------------
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils
max_frames = 30     # 30í”„ë ˆì„ (25~30ìœ¼ë¡œ ìœ ë™ì  ì¡°ì • ê°€ëŠ¥)
pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, min_detection_confidence=0.5)

# ê´€ì ˆ ì¸ë±ìŠ¤ê°€ ë¶€ì¡±í•  ì‹œ -1.0ìœ¼ë¡œ ì²˜ë¦¬
def extract_20joints_from_landmarks(landmarks, joint_map):
    joint_data = []
    for joint_name, info in joint_map.items():
        if isinstance(info, tuple) and info[0] == "avg":
            indices = info[1]
            if all(i < len(landmarks) for i in indices):
                joint = np.mean([landmarks[i] for i in indices], axis=0)
            else:
                joint = np.array([-1.0, -1.0, -1.0])
        else:
            if info < len(landmarks):
                joint = landmarks[info]
            else:
                joint = np.array([-1.0, -1.0, -1.0])
        joint_data.extend(joint.tolist())
    return joint_data

#------------------ ê´€ì ˆ ë§¤í•‘ (20ê°œ) ------------------
mediapipe_index = {
    "Head": 0,
    "Shouldercentre": ("avg", [11, 14]),
    "Spine": ("avg", [27, 28]),
    "Leftshoulder": 14,
    "Leftelbow": 15,
    "Lefthand": 16,
    "Rightshoulder": 11,
    "Rightelbow": 12,
    "Righthand": 13,
    "Lefthip": 27,
    "Leftknee": 29,
    "Leftfoot": 31,
    "Righthip": 28,
    "Rightknee": 30,
    "Rightfoot": 32,
    "Hipcentre": ("avg", [27, 28]),
    "Leftwrist": 16,
    "Leftankle": 31,
    "Rightwrist": 13,
    "Rightankle": 32
}

#------------------ LSTM ì˜¤í† ì¸ì½”ë” ------------------
class LSTMAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=2, dropout=0.3, batch_first=True)
        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers=2, dropout=0.3, batch_first=True)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.output_layer = nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        _, (h_n, c_n) = self.encoder(x)
        decoder_input = torch.zeros(batch_size, seq_len, h_n.shape[-1]).to(x.device)
        dec_out, _ = self.decoder(decoder_input, (h_n, c_n))
        dec_out = self.layer_norm(dec_out)  # ì•ˆì •í™”
        out = self.output_layer(dec_out)

        return out

#------------------ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ------------------ 
input_dim = 60  # 20 joints * 3
hidden_dim = 64
seq_len = 30

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMAutoencoder(input_dim, hidden_dim).to(device)
model.load_state_dict(torch.load("/home/py/lstm_autoencoder.pth", map_location=device)) #íŒŒì¼ê²½ë¡œ
model.eval()

#------------------ ìœ„í—˜ ë™ì‘ ê°ì§€ ------------------ 
N_JOINTS = 20  # ê´€ì ˆ ê°œìˆ˜
SEQ_LEN = 30   # 30í”„ë ˆì„
STATE_NORMAL = 0       # ì„œìˆìŒ
STATE_LYING = 1        # ëˆ„ì›ŒìˆëŠ” ìƒíƒœ
STATE_POSSIBLE_FALL = 2  # ë‚™ìƒ ì˜ì‹¬ (ëˆ„ì›Œì„œ ì •ì§€)

def estimate_posture_from_bbox(bbox, standing_thresh=1.2, lying_thresh=0.8):
    x1, y1, x2, y2 = bbox
    width = abs(x2 - x1)
    height = abs(y2 - y1)

    # ì˜ˆì™¸ ì²˜ë¦¬: bboxê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì„ ê²½ìš°
    if width <= 0 or height <= 0:
        return "invalid"

    aspect_ratio = height / width  # ë¹„ìœ¨ ê³„ì‚°

    if aspect_ratio > standing_thresh:
        return "standing"
    elif aspect_ratio < lying_thresh:
        return "lying"
    else:
        return "ambiguous"

def fall_detection_fsm(posture, motion_score, state, counter,
                       motion_thresh=0.01, lying_duration_thresh=30):
    is_fall = False

    if posture == "lying" and motion_score < motion_thresh:
        counter += 1
        if counter >= lying_duration_thresh:
            state = STATE_POSSIBLE_FALL
            is_fall = True
    else:
        state = STATE_NORMAL
        counter = 0

    return state, counter, is_fall

def process_single_frame(image, frame_idx):

    # ì‚¬ëŒ ë°•ìŠ¤ ì¶”ì¶œ
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = yolo_model(image_rgb)
    boxes = results.xyxy[0].cpu().numpy()

    # bboxì¶”ì¶œ ì•ˆë˜ë©´ ë‹¤ìŒ í”„ë ˆì„ìœ¼ë¡œ ë„˜ì–´ê°
    if len(boxes) == 0:
        log_status_every_3s(frame_idx, detected=False, person_id=None)
        return

    # ê´€ì ˆ ì¶”ì¶œ
    for pid, det in enumerate(boxes):
        if int(det[5]) != 0:
            continue

        x1, y1, x2, y2, _, cls = map(int, det[:6])
        bbox = [x1, y1, x2, y2]
        crop = image_rgb[y1:y2, x1:x2]
        res = pose.process(crop)

        if res.pose_landmarks:
            landmarks = np.array([[lm.x, lm.y, lm.visibility] for lm in res.pose_landmarks.landmark])
            # ìœ„ì—ì„œ ì •ì˜í•œ 20ê°€ì§€ ê´€ì ˆ ì¶”ì¶œ
            joints = extract_20joints_from_landmarks(landmarks, mediapipe_index)
            log_status_every_3s(frame_idx, True, pid)
        else:
            # ê´€ì ˆ ì‹¤íŒ¨ â†’ bboxë§Œ ì¡´ì¬
            # bboxë¡œ ì‚¬ëŒì€ ì¶”ì¶œëëŠ”ë° ê´€ì ˆì€ ì¶”ì¶œ ì•ˆëœ ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬.
            # bboxë¡œ ì‚¬ëŒ ì¶”ì¶œë˜ëŠ” ê²ƒì´ ê´€ì ˆ ì¶”ì¶œë˜ëŠ” ê²ƒë³´ë‹¤ ë§ì•„ì„œ bboxë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ ì“°ì„
            joints = [-1.0] * (N_JOINTS * 3)
            log_status_every_3s(frame_idx, True, pid)

        # ê´€ì ˆ ì €ì¥
        joints_np = np.array(joints).reshape(N_JOINTS, 3)

        # ìµœì´ˆ ê°ì§€ì ì´ˆê¸°í™”
        if pid not in person_state:
            person_state[pid] = {'state': STATE_NORMAL, 'counter': 0, 'prev_bbox': bbox}
            person_prev_joints[pid] = None
            person_joint_buffer[pid] = []

        # ëª¨ì…˜ìŠ¤ì½”ì–´ ê³„ì‚° (ê´€ì ˆ ê¸°ë°˜)
        if person_prev_joints[pid] is not None and (joints_np[:, 0] != -1.0).all():
            diff = np.linalg.norm(joints_np[:, :2] - person_prev_joints[pid][:, :2], axis=1)
            score = float(np.mean(diff))
        else:
            score = 0.0
        person_prev_joints[pid] = joints_np

        # ìì„¸ ì¶”ì • ë° ë‚™ìƒ FSM
        posture = estimate_posture_from_bbox(bbox)
        state, counter, is_fall = fall_detection_fsm(posture, score, person_state[pid]['state'], person_state[pid]['counter'])
        person_state[pid]['state'] = state
        person_state[pid]['counter'] = counter
        person_state[pid]['prev_bbox'] = bbox

        if is_fall:
            alert_buzzer_and_led()
            print(f"âš ï¸ ë‚™ìƒ ì˜ì‹¬: Frame {frame_idx}, ID={pid}, posture={posture}, motion_score={score:.4f}")

        # AutoEncoder ì´ìƒ ë™ì‘ ê°ì§€ (ë™ì  ìƒíƒœì—ì„œë§Œ)
        static_flag = 0 if score < 0.02 else 1
        if static_flag == 1 and (joints_np[:, 0] != -1.0).all():
            flat = joints_np.flatten()   # (20 x 3) â†’ (60,)
            person_joint_buffer[pid].append(flat)
            if len(person_joint_buffer[pid]) > SEQ_LEN:
                person_joint_buffer[pid].pop(0)
            if len(person_joint_buffer[pid]) == SEQ_LEN:
                input_seq = torch.tensor(person_joint_buffer[pid], dtype=torch.float32).unsqueeze(0).to(device)
                with torch.no_grad():
                    recon = model(input_seq)
                    recon_error = torch.mean((input_seq - recon) ** 2).item()
                print(f"Frame {frame_idx}: ID={pid} ë™ì ìƒíƒœ â†’ ë³µì›ì˜¤ì°¨ MSE = {recon_error:.5f}")
                if recon_error > 0.01:  # ì„ê³„ê°’
                    alert_buzzer_and_led()
                    print(f"ì´ìƒ ë™ì‘ ê°ì§€ (autoencoder): ID={pid}, MSE = {recon_error:.5f}")

#------------------ í”„ë ˆì„ ì¶”ì¶œ ------------------
#cap = cv2.VideoCapture(0)# ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ì…ë ¥
#fps = cap.get(cv2.CAP_PROP_FPS)
#interval = int(round(fps / 30))  # 30FPS ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ ê°„ê²© ê³„ì‚°

#frames = []
#frame_ids = []
#count = 0
#while cap.isOpened():
#    ret, frame = cap.read()
#    if not ret:
#        break
#    if count % interval == 0:
#        resized = cv2.resize(frame, (640, 480))
#        frames.append(resized)
#        frame_ids.append(count)
#    count += 1
#cap.release()
# ffmpeg ëª…ë ¹ì–´ (libcameraë¡œ ì‹¤ì‹œê°„ ì˜ìƒ ì½ì–´ì„œ OpenCVì— ë„˜ê¹€)


ffmpeg_cmd = [
    "ffmpeg",
    "-f", "v4l2",              # Video4Linux2
    "-framerate", "30",
    "-video_size", f"{WIDTH}x{HEIGHT}",
    "-i", "/dev/video0",       # ë¼ì¦ˆë² ë¦¬íŒŒì´ ì¹´ë©”ë¼ ì¥ì¹˜ (libcamera-vidê°€ /dev/video0ìœ¼ë¡œ ë¼ìš°íŒ…ë¨)
    "-f", "rawvideo",
    "-pix_fmt", "bgr24",
    "pipe:1"
]

# ffmpeg subprocess ì‹¤í–‰
def main():
    proc = subprocess.Popen(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)

    frames = []
    frame_ids = []
    count = 0
    interval = 1  # 30FPS ê¸°ì¤€ ì „ì œ í•˜ì— ë§¤ í”„ë ˆì„ ì¶”ì¶œ

    try:
        while True:
            # í•œ í”„ë ˆì„ ì½ê¸°
            raw_frame = proc.stdout.read(WIDTH * HEIGHT * 3)
            if not raw_frame:
                break

            frame = np.frombuffer(raw_frame, np.uint8).reshape((HEIGHT, WIDTH, 3))

            if count % interval == 0:
                resized = cv2.resize(frame, (640, 480))
                frames.append(resized)
                frame_ids.append(count)
                process_single_frame(resized, count)

            count += 1

            # í”„ë¦¬ë·° ë„ìš°ê³  'q' í‚¤ ëˆ„ë¥´ë©´ ì¢…ë£Œ
            cv2.imshow("ğŸ“· Live Stream", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    except KeyboardInterrupt:
        print("ì‚¬ìš©ìì— ì˜í•´ ì¢…ë£Œë¨.")

    finally:
        proc.terminate()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()